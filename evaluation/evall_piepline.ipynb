{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2d95c8",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8199c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from peft import PeftModel\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776ae46d",
   "metadata": {},
   "source": [
    "## Evaluation on the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f9eb49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"unsloth/llama-3.2-1b-instruct\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1235175e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging PEFT weights into base model…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge complete.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    \"../finetuned_model_weights/run2\",\n",
    "    is_trainable=False,\n",
    "    local_files_only=True   \n",
    ")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Merging PEFT weights into base model…\")\n",
    "model = model.merge_and_unload()\n",
    "print(\"Merge complete.\")\n",
    "\n",
    "gen_config = GenerationConfig(\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c93cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"../data/combined_val_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dced98d",
   "metadata": {},
   "source": [
    "### Evaluation on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9acd1726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [06:31<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exact‐match count: 427/500\n",
      "\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89       166\n",
      "    positive       0.87      0.85      0.86       167\n",
      "     neutral       0.79      0.85      0.82       167\n",
      "\n",
      "    accuracy                           0.85       500\n",
      "   macro avg       0.86      0.85      0.85       500\n",
      "weighted avg       0.86      0.85      0.85       500\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "[[143   6  17]\n",
      " [  4 142  21]\n",
      " [  9  16 142]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_words = [\"negative\", \"positive\", \"neutral\"]\n",
    "preds, actuals = [], []\n",
    "count = 0\n",
    "\n",
    "# loop over entire val set\n",
    "for _, row in tqdm(eval_df.iterrows(), total=len(eval_df)):\n",
    "      messages = [{\"role\": \"user\", \"content\": row[\"user_msg\"]}]\n",
    "      prompt = tokenizer.apply_chat_template(\n",
    "          messages, tokenize=False, add_generation_prompt=True\n",
    "      )\n",
    "      inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "      outputs = model.generate(**inputs, generation_config=gen_config)\n",
    "      response = tokenizer.decode(outputs[0], skip_special_tokens=True).lower()\n",
    "\n",
    "      last_word = response.split()[-1].strip(\".,!?;:\")\n",
    "      pred = last_word\n",
    "      preds.append(pred)\n",
    "      actuals.append(row[\"output\"])\n",
    "      if pred == row[\"output\"]:\n",
    "          count += 1\n",
    "\n",
    "print(f\"\\nExact‐match count: {count}/{len(preds)}\\n\")\n",
    "\n",
    "print(\"CLASSIFICATION REPORT:\")\n",
    "print(classification_report(actuals, preds, labels=sentiment_words))\n",
    "\n",
    "print(\"CONFUSION MATRIX:\")\n",
    "print(confusion_matrix(actuals, preds, labels=sentiment_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95fbb4b",
   "metadata": {},
   "source": [
    "### Evaluation on english and vietnamese respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a0615ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "English examples: 250\n"
     ]
    }
   ],
   "source": [
    "eval_df_eng = eval_df[eval_df[\"language\"] == \"eng\"]\n",
    "print(f\"\\nEnglish examples: {len(eval_df_eng)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "861aa937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:51<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exact‐match count: 204/250\n",
      "\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.84      0.87        83\n",
      "    positive       0.81      0.77      0.79        84\n",
      "     neutral       0.75      0.83      0.79        83\n",
      "\n",
      "    accuracy                           0.82       250\n",
      "   macro avg       0.82      0.82      0.82       250\n",
      "weighted avg       0.82      0.82      0.82       250\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "[[70  6  7]\n",
      " [ 3 65 16]\n",
      " [ 5  9 69]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_words = [\"negative\", \"positive\", \"neutral\"]\n",
    "preds, actuals = [], []\n",
    "count = 0\n",
    "\n",
    "# loop over eng val set\n",
    "for _, row in tqdm(eval_df_eng.iterrows(), total=len(eval_df_eng)):\n",
    "      messages = [{\"role\": \"user\", \"content\": row[\"user_msg\"]}]\n",
    "      prompt = tokenizer.apply_chat_template(\n",
    "          messages, tokenize=False, add_generation_prompt=True\n",
    "      )\n",
    "      inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "      outputs = model.generate(**inputs, generation_config=gen_config)\n",
    "      response = tokenizer.decode(outputs[0], skip_special_tokens=True).lower()\n",
    "\n",
    "      last_word = response.split()[-1].strip(\".,!?;:\")\n",
    "      pred = last_word\n",
    "      preds.append(pred)\n",
    "      actuals.append(row[\"output\"])\n",
    "      if pred == row[\"output\"]:\n",
    "          count += 1\n",
    "\n",
    "print(f\"\\nExact‐match count: {count}/{len(preds)}\\n\")\n",
    "\n",
    "print(\"CLASSIFICATION REPORT:\")\n",
    "print(classification_report(actuals, preds, labels=sentiment_words))\n",
    "\n",
    "print(\"CONFUSION MATRIX:\")\n",
    "print(confusion_matrix(actuals, preds, labels=sentiment_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77795e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vietnamese examples: 250\n"
     ]
    }
   ],
   "source": [
    "eval_df_viet = eval_df[eval_df[\"language\"] == \"vi\"]\n",
    "print(f\"\\nVietnamese examples: {len(eval_df_viet)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "895373e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:57<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exact‐match count: 222/250\n",
      "\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.88      0.90        83\n",
      "    positive       0.92      0.93      0.92        83\n",
      "     neutral       0.83      0.86      0.84        84\n",
      "\n",
      "    accuracy                           0.89       250\n",
      "   macro avg       0.89      0.89      0.89       250\n",
      "weighted avg       0.89      0.89      0.89       250\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "[[73  0 10]\n",
      " [ 1 77  5]\n",
      " [ 5  7 72]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_words = [\"negative\", \"positive\", \"neutral\"]\n",
    "preds, actuals = [], []\n",
    "count = 0\n",
    "\n",
    "# loop over entire val set\n",
    "for _, row in tqdm(eval_df_viet.iterrows(), total=len(eval_df_viet)):\n",
    "      messages = [{\"role\": \"user\", \"content\": row[\"user_msg\"]}]\n",
    "      prompt = tokenizer.apply_chat_template(\n",
    "          messages, tokenize=False, add_generation_prompt=True\n",
    "      )\n",
    "      inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "      outputs = model.generate(**inputs, generation_config=gen_config)\n",
    "      response = tokenizer.decode(outputs[0], skip_special_tokens=True).lower()\n",
    "\n",
    "      last_word = response.split()[-1].strip(\".,!?;:\")\n",
    "      pred = last_word\n",
    "      preds.append(pred)\n",
    "      actuals.append(row[\"output\"])\n",
    "      if pred == row[\"output\"]:\n",
    "          count += 1\n",
    "\n",
    "print(f\"\\nExact‐match count: {count}/{len(preds)}\\n\")\n",
    "\n",
    "print(\"CLASSIFICATION REPORT:\")\n",
    "print(classification_report(actuals, preds, labels=sentiment_words))\n",
    "\n",
    "print(\"CONFUSION MATRIX:\")\n",
    "print(confusion_matrix(actuals, preds, labels=sentiment_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64f0dc2",
   "metadata": {},
   "source": [
    "## Evaluation on Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d32529a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# 4) Prepare generation config\n",
    "gen_config = GenerationConfig(\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61c0eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da359dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentiment_from_response(response: str) -> str:\n",
    "    \"\"\"\n",
    "    1) Drop everything up to and including the first 'assistant'\n",
    "    2) Find and return the first sentiment token (negative|neutral|positive)\n",
    "    \"\"\"\n",
    "    # 1) Split on 'assistant' and keep the tail\n",
    "    parts = response.split(\"assistant\", 1)\n",
    "    tail = parts[1] if len(parts) > 1 else response\n",
    "\n",
    "    # 2) Search for sentiment words\n",
    "    match = re.search(r'\\b(negative|neutral|positive)\\b', tail.lower())\n",
    "    return match.group(1) if match else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d0b88e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [18:52<00:00,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exact‐match count: 212/500\n",
      "\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.42      0.48       166\n",
      "    positive       0.47      0.41      0.44       167\n",
      "     neutral       0.36      0.44      0.40       167\n",
      "\n",
      "   micro avg       0.45      0.42      0.44       500\n",
      "   macro avg       0.47      0.42      0.44       500\n",
      "weighted avg       0.47      0.42      0.44       500\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "[[70 27 60]\n",
      " [23 68 69]\n",
      " [32 49 74]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_words = [\"negative\", \"positive\", \"neutral\"]\n",
    "preds, actuals = [], []\n",
    "count = 0\n",
    "\n",
    "# loop over entire val set\n",
    "for _, row in tqdm(eval_df.iterrows(), total=len(eval_df)):\n",
    "      messages = [{\"role\": \"user\", \"content\": row[\"user_msg\"]}]\n",
    "      prompt = tokenizer.apply_chat_template(\n",
    "          messages, tokenize=False, add_generation_prompt=True\n",
    "      )\n",
    "      inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "      outputs = model_base.generate(**inputs, generation_config=gen_config)\n",
    "      response = tokenizer.decode(outputs[0], skip_special_tokens=True).lower()\n",
    "\n",
    "      pred = extract_sentiment_from_response(response)\n",
    "      preds.append(pred)\n",
    "      actuals.append(row[\"output\"])\n",
    "      if pred == row[\"output\"]:\n",
    "          count += 1\n",
    "\n",
    "print(f\"\\nExact‐match count: {count}/{len(preds)}\\n\")\n",
    "\n",
    "print(\"CLASSIFICATION REPORT:\")\n",
    "print(classification_report(actuals, preds, labels=sentiment_words))\n",
    "\n",
    "print(\"CONFUSION MATRIX:\")\n",
    "print(confusion_matrix(actuals, preds, labels=sentiment_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecdd56f",
   "metadata": {},
   "source": [
    "### Evcaluate on English and Vietnamese Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff834806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [14:29<00:00,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exact‐match count: 52/250\n",
      "\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.22      0.34        83\n",
      "    positive       0.52      0.32      0.40        84\n",
      "     neutral       1.00      0.08      0.16        83\n",
      "\n",
      "   micro avg       0.64      0.21      0.31       250\n",
      "   macro avg       0.78      0.21      0.30       250\n",
      "weighted avg       0.78      0.21      0.30       250\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "[[18 11  0]\n",
      " [ 1 27  0]\n",
      " [ 3 14  7]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_words = [\"negative\", \"positive\", \"neutral\"]\n",
    "preds, actuals = [], []\n",
    "count = 0\n",
    "\n",
    "# loop over eng val set\n",
    "for _, row in tqdm(eval_df_eng.iterrows(), total=len(eval_df_eng)):\n",
    "      messages = [{\"role\": \"user\", \"content\": row[\"user_msg\"]}]\n",
    "      prompt = tokenizer.apply_chat_template(\n",
    "          messages, tokenize=False, add_generation_prompt=True\n",
    "      )\n",
    "      inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "      outputs = model_base.generate(**inputs, generation_config=gen_config)\n",
    "      response = tokenizer.decode(outputs[0], skip_special_tokens=True).lower()\n",
    "\n",
    "      last_word = response.split()[-1].strip(\".,!?;:\")\n",
    "      pred = last_word\n",
    "      preds.append(pred)\n",
    "      actuals.append(row[\"output\"])\n",
    "      if pred == row[\"output\"]:\n",
    "          count += 1\n",
    "\n",
    "print(f\"\\nExact‐match count: {count}/{len(preds)}\\n\")\n",
    "\n",
    "print(\"CLASSIFICATION REPORT:\")\n",
    "print(classification_report(actuals, preds, labels=sentiment_words))\n",
    "\n",
    "print(\"CONFUSION MATRIX:\")\n",
    "print(confusion_matrix(actuals, preds, labels=sentiment_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9af9733e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [06:00<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exact‐match count: 77/250\n",
      "\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      0.16      0.24        83\n",
      "    positive       0.80      0.05      0.09        83\n",
      "     neutral       0.33      0.71      0.45        84\n",
      "\n",
      "   micro avg       0.36      0.31      0.33       250\n",
      "   macro avg       0.55      0.31      0.26       250\n",
      "weighted avg       0.55      0.31      0.26       250\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "[[13  0 57]\n",
      " [ 2  4 67]\n",
      " [10  1 60]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_words = [\"negative\", \"positive\", \"neutral\"]\n",
    "preds, actuals = [], []\n",
    "count = 0\n",
    "\n",
    "# loop over entire val set\n",
    "for _, row in tqdm(eval_df_viet.iterrows(), total=len(eval_df_viet)):\n",
    "      messages = [{\"role\": \"user\", \"content\": row[\"user_msg\"]}]\n",
    "      prompt = tokenizer.apply_chat_template(\n",
    "          messages, tokenize=False, add_generation_prompt=True\n",
    "      )\n",
    "      inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "      outputs = model_base.generate(**inputs, generation_config=gen_config)\n",
    "      response = tokenizer.decode(outputs[0], skip_special_tokens=True).lower()\n",
    "\n",
    "      last_word = response.split()[-1].strip(\".,!?;:\")\n",
    "      pred = last_word\n",
    "      preds.append(pred)\n",
    "      actuals.append(row[\"output\"])\n",
    "      if pred == row[\"output\"]:\n",
    "          count += 1\n",
    "\n",
    "print(f\"\\nExact‐match count: {count}/{len(preds)}\\n\")\n",
    "\n",
    "print(\"CLASSIFICATION REPORT:\")\n",
    "print(classification_report(actuals, preds, labels=sentiment_words))\n",
    "\n",
    "print(\"CONFUSION MATRIX:\")\n",
    "print(confusion_matrix(actuals, preds, labels=sentiment_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
