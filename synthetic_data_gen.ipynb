{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92fdfcb6",
   "metadata": {},
   "source": [
    "# 1.2 Synthetic Data Generation\n",
    "The main logic for my generation goes as folows:\n",
    "1. Have a defined sentiment for all of my prompting as a baseline for generating the input and output\n",
    "    - The generation is a diversed mix of zero shot and few shot prompting on the model to strike a balance between the variety of the prompts\n",
    "    - Prompt is requested to generate of around 5-30 words which covers the 25% to 75% percentile to ensure a diverse but not obsecure range of data is generated\n",
    "    - The direct request to generate inputs of a certain sentiment ensures the initial generation to already have a more or less accurate output\n",
    "    - This is done as I realise that the model is rather weak and requires extensive handhelding in generation\n",
    "2. Have a very small sample(of about 30 samples) of real life data inputs from my peers to make the data more diverse again and also provide a sense of human authenticity to it\n",
    "3. Have a checker but #TO-DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af75d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea0f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6219dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate user inputs\n",
    "def generate_user_input(shot, sentiment, type_of_input):\n",
    "    \"\"\"\n",
    "    Generate user input for the model based on the type of input and sentiment.\n",
    "    \n",
    "    Args:\n",
    "        shot (int): either 0 or 1, indicating whether to provide an example or not.\n",
    "        sentiment (str): The sentiment of the headline (neutral, positive, negative).\n",
    "        type_of_input (str): The type of input to generate (e.g., 'headline').\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated input string.\n",
    "    \"\"\"\n",
    "    output = f\"Generate a financial {type_of_input} with {sentiment} sentiment. \"\n",
    "    output += f\"The {type_of_input} should be 5-30 words, factual in tone. \"\n",
    "    \n",
    "    if shot == 0:\n",
    "        output += f\"Example: this is the {type_of_input} you generated\"\n",
    "        return output\n",
    "    \n",
    "    examples = {\n",
    "        \"neutral\": \"Example: A Sexist Joke Cost Ken Fisher $4 Billion in Assets. He Still Runs $121 Billion.\\n\",\n",
    "        \"positive\": \"Example: Western Union will be working with MercadoLibre, the South American eCommerce giant, so digital remittances can be sent in Mexico.\\n\",\n",
    "        \"negative\": \"Example: The app will be delisted from Apple's App Store on Oct. 5.\\n\"\n",
    "    }\n",
    "    \n",
    "    output += examples.get(sentiment, \"\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e2d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a input for the dataset\n",
    "def generate_synthetic_input(prompt):\n",
    "    \"\"\"\n",
    "    Generate synthetic data using the provided prompt with the normal model and tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        prompt (list or str): The input prompt for the model.\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated text from the model.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs, \n",
    "        max_new_tokens=200,\n",
    "        use_cache=True,\n",
    "        temperature=0.9, \n",
    "        min_p=0.1\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4d81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to clean the generated text\n",
    "def extract_headline(generated_text):\n",
    "    \"\"\"\n",
    "    Extract a clean headline from the generated text.\n",
    "    \n",
    "    Args:\n",
    "        generated_text (str): Raw generated text from the model\n",
    "        \n",
    "    Returns:\n",
    "        str: Clean headline without formatting\n",
    "    \"\"\"\n",
    "    # Remove all EOT tags (in various forms)\n",
    "    generated_text = re.sub(r'<\\|eot(?:_id)?\\|>', '', generated_text)\n",
    "    \n",
    "    # Look for content after assistant tag\n",
    "    if \"<|start_header_id|>assistant<|end_header_id|>\" in generated_text:\n",
    "        content = generated_text.split(\"<|start_header_id|>assistant<|end_header_id|>\")[1].strip()\n",
    "    else:\n",
    "        content = generated_text\n",
    "    \n",
    "    # Find numbered headlines (like \"1. Headline text\")\n",
    "    lines = content.split('\\n')\n",
    "    for line in lines:\n",
    "        # Match lines like \"1. Headline\" or \"1. **Headline**\"\n",
    "        match = re.search(r'\\d+\\.\\s+(?:\\*\\*)?([^*\\n]+)(?:\\*\\*)?', line)\n",
    "        if match:\n",
    "            headline = match.group(1).strip()\n",
    "            # Remove any remaining special tokens\n",
    "            headline = re.sub(r'<\\|[^|]+\\|>', '', headline)\n",
    "            return headline\n",
    "    \n",
    "    # If no numbered headlines found, just return first non-empty line\n",
    "    for line in lines:\n",
    "        if line.strip() and not line.startswith(\"<|\") and not line.startswith(\"Here are\"):\n",
    "            # Remove any remaining special tokens\n",
    "            clean_line = re.sub(r'<\\|[^|]+\\|>', '', line.strip())\n",
    "            return clean_line\n",
    "    \n",
    "    # Clean the content as a last resort\n",
    "    clean_content = re.sub(r'<\\|[^|]+\\|>', '', content.strip())\n",
    "    return clean_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbcc3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main logic to generate synthetic data\n",
    "\n",
    "sentiments = [\"neutral\", \"positive\", \"negative\"]\n",
    "type_of_shot = [0, 1]\n",
    "\n",
    "synthetic_data = []\n",
    "n = 0  # Counter for the number of generated headlines\n",
    "\n",
    "for sentiment in sentiments:\n",
    "    for shot in type_of_shot:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a market news generator. Generate ONLY the headline text. Do NOT include any introductions, explanations, or formatting instructions. Do NOT write phrases like 'Here's a headline' or 'Market headline:'. Just output the clean headline text directly.\"},\n",
    "            {\"role\": \"user\", \"content\": generate_user_input(shot, sentiment, \"headline\")},\n",
    "        ]\n",
    "\n",
    "        i = 0\n",
    "        \n",
    "        while i < 79:\n",
    "            if n > 1:\n",
    "                break\n",
    "            print(\"generating \", n)\n",
    "            generated_text = generate_synthetic_input(messages)\n",
    "        \n",
    "            # Extract just the headline\n",
    "            headline = extract_headline(generated_text)\n",
    "            \n",
    "            synthetic_data.append({\n",
    "                    \"input\": headline,\n",
    "                    \"output\": sentiment,\n",
    "                    \"instruction\": \"Base on the sentiment of the headline, classify it as neutral, positive, or negative.\"\n",
    "                })\n",
    "            print(headline)\n",
    "            i += 1\n",
    "            n += 1\n",
    "\n",
    "print(\"Done generating lol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8741a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "df = pd.DataFrame(synthetic_data)\n",
    "csv_path = \"data/synthetic_data_generate.csv\"\n",
    "\n",
    "# Check if file exists and append if it does\n",
    "if os.path.exists(csv_path):\n",
    "    # Load existing data\n",
    "    existing_df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Combine existing and new data\n",
    "    combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    \n",
    "    # Save the combined data\n",
    "    combined_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Appended {len(df)} new entries to existing data. Total entries: {len(combined_df)}\")\n",
    "else:\n",
    "    # If file doesn't exist yet, just save the new data\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Created new file with {len(df)} entries\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
