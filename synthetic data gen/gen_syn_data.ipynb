{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92fdfcb6",
   "metadata": {},
   "source": [
    "# 1.2 Synthetic Data Generation\n",
    "Main logic for my generation:\n",
    "1. Have a defined sentiment for all of my prompting as a baseline for generating the input and output\n",
    "    - The generation is a diversed mix of zero shot and few shot prompting on the model to strike a balance between the variety of the prompts\n",
    "    - Prompt is requested to generate of around 5-30 words which covers the 25% to 75% percentile to ensure a diverse but not obsecure range of data is generated\n",
    "    - The direct request to generate inputs of a certain sentiment ensures the initial generation to already have a more or less accurate output\n",
    "    - This is done as I realise that the model is rather weak and requires extensive handhelding in generation\n",
    "2. Have a very small sample(of about 30 samples) of real life data inputs from my peers to make the data more diverse again and also provide a sense of human authenticity to it\n",
    "3. Evaluation is done with a heuristic to make sure the data generated is of certain quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e4708",
   "metadata": {},
   "source": [
    "## Main Logic to Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af75d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4ea0f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0e66eb",
   "metadata": {},
   "source": [
    "### Function to generate user inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6219dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_input(shot, sentiment, type_of_input):\n",
    "    \"\"\"\n",
    "    Generate user input for the model based on the type of input and sentiment.\n",
    "    \n",
    "    Args:\n",
    "        shot (int): either 0 or 1, indicating whether to provide an example or not.\n",
    "        sentiment (str): The sentiment of the headline (neutral, positive, negative).\n",
    "        type_of_input (str): The type of input to generate (e.g., 'headline').\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated input string.\n",
    "    \"\"\"\n",
    "    output = f\"Generate a financial {type_of_input} with {sentiment} sentiment. \"\n",
    "    output += f\"The {type_of_input} should be 5-30 words, factual in tone. \"\n",
    "    \n",
    "    if shot == 0:\n",
    "        output += f\"Example: this is the {type_of_input} you generated\"\n",
    "        return output\n",
    "    \n",
    "    examples = {\n",
    "        \"neutral\": \"Example: A Sexist Joke Cost Ken Fisher $4 Billion in Assets. He Still Runs $121 Billion.\\n\",\n",
    "        \"positive\": \"Example: Western Union will be working with MercadoLibre, the South American eCommerce giant, so digital remittances can be sent in Mexico.\\n\",\n",
    "        \"negative\": \"Example: The app will be delisted from Apple's App Store on Oct. 5.\\n\"\n",
    "    }\n",
    "    \n",
    "    output += examples.get(sentiment, \"\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f88dda",
   "metadata": {},
   "source": [
    "### Function to generate a input for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e2d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_input(prompt):\n",
    "    \"\"\"\n",
    "    Generate synthetic data using the provided prompt with the normal model and tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        prompt (list or str): The input prompt for the model.\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated text from the model.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs, \n",
    "        max_new_tokens=200,\n",
    "        use_cache=True,\n",
    "        temperature=0.9, \n",
    "        min_p=0.1\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c37cb33",
   "metadata": {},
   "source": [
    "### Function to extract out the exact headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f4d81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to clean the generated text\n",
    "def extract_headline(generated_text):\n",
    "    \"\"\"\n",
    "    Extract a clean headline from the generated text.\n",
    "    \n",
    "    Args:\n",
    "        generated_text (str): Raw generated text from the model\n",
    "        \n",
    "    Returns:\n",
    "        str: Clean headline without formatting\n",
    "    \"\"\"\n",
    "    # Remove all EOT tags (in various forms)\n",
    "    generated_text = re.sub(r'<\\|eot(?:_id)?\\|>', '', generated_text)\n",
    "    \n",
    "    # Look for content after assistant tag\n",
    "    if \"<|start_header_id|>assistant<|end_header_id|>\" in generated_text:\n",
    "        content = generated_text.split(\"<|start_header_id|>assistant<|end_header_id|>\")[1].strip()\n",
    "    else:\n",
    "        content = generated_text\n",
    "    \n",
    "    # Find numbered headlines (like \"1. Headline text\")\n",
    "    lines = content.split('\\n')\n",
    "    for line in lines:\n",
    "        # Match lines like \"1. Headline\" or \"1. **Headline**\"\n",
    "        match = re.search(r'\\d+\\.\\s+(?:\\*\\*)?([^*\\n]+)(?:\\*\\*)?', line)\n",
    "        if match:\n",
    "            headline = match.group(1).strip()\n",
    "            # Remove any remaining special tokens\n",
    "            headline = re.sub(r'<\\|[^|]+\\|>', '', headline)\n",
    "            return headline\n",
    "    \n",
    "    # If no numbered headlines found, just return first non-empty line\n",
    "    for line in lines:\n",
    "        if line.strip() and not line.startswith(\"<|\") and not line.startswith(\"Here are\"):\n",
    "            # Remove any remaining special tokens\n",
    "            clean_line = re.sub(r'<\\|[^|]+\\|>', '', line.strip())\n",
    "            return clean_line\n",
    "    \n",
    "    # Clean the content as a last resort\n",
    "    clean_content = re.sub(r'<\\|[^|]+\\|>', '', content.strip())\n",
    "    return clean_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e6526",
   "metadata": {},
   "source": [
    "### Main Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbcc3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating  0\n",
      "U.S. GDP Growth Slows to 2.3% in Q2, Reaching 61.4 Trillion Dollar Mark\n",
      "generating  1\n",
      "US Job Market Grows, Consumer Prices Rise Amidst Slow Economic Growth\n",
      "Done generating lol\n"
     ]
    }
   ],
   "source": [
    "sentiments = [\"neutral\", \"positive\", \"negative\"]\n",
    "type_of_shot = [0, 1]\n",
    "\n",
    "synthetic_data = []\n",
    "n = 0  # Counter for the number of generated headlines\n",
    "\n",
    "for sentiment in sentiments:\n",
    "    for shot in type_of_shot:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a market news generator. Generate ONLY the headline text. Do NOT include any introductions, explanations, or formatting instructions. Do NOT write phrases like 'Here's a headline' or 'Market headline:'. Just output the clean headline text directly.\"},\n",
    "            {\"role\": \"user\", \"content\": generate_user_input(shot, sentiment, \"headline\")},\n",
    "        ]\n",
    "\n",
    "        i = 0\n",
    "        \n",
    "        while i < 79:\n",
    "            if n > 470:\n",
    "                break\n",
    "            print(\"generating \", n)\n",
    "            generated_text = generate_synthetic_input(messages)\n",
    "        \n",
    "            # Extract just the headline\n",
    "            headline = extract_headline(generated_text)\n",
    "            \n",
    "            synthetic_data.append({\n",
    "                    \"input\": headline,\n",
    "                    \"output\": sentiment,\n",
    "                    \"instruction\": \"Base on the sentiment of the headline, classify it as neutral, positive, or negative.\"\n",
    "                })\n",
    "            print(headline)\n",
    "            i += 1\n",
    "            n += 1\n",
    "\n",
    "print(\"Done generating lol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b7457c",
   "metadata": {},
   "source": [
    "## Validation of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87925bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "import language_tool_python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 0) select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1) grammar checker\n",
    "tool = language_tool_python.LanguageTool(\"en-US\")\n",
    "\n",
    "# 2) load LM for scoring\n",
    "tok_pp = AutoTokenizer.from_pretrained(\"unsloth/llama-3.2-1b-instruct\")\n",
    "model_pp = AutoModelForCausalLM.from_pretrained(\n",
    "    \"unsloth/llama-3.2-1b-instruct\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ").to(device).eval()\n",
    "\n",
    "def perplexity(sentence):\n",
    "    enc = tok_pp(sentence, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model_pp(**enc, labels=enc[\"input_ids\"])\n",
    "    return torch.exp(out.loss).item()\n",
    "\n",
    "def is_high_quality(headline):\n",
    "    # A) length heuristics\n",
    "    w = headline.split()\n",
    "    if len(w) < 5 or len(w) > 30:\n",
    "        return False\n",
    "    # B) ban URLs or stray tokens\n",
    "    if re.search(r\"http[s]?://|\\<\\|[^\\|]+\\|\\>\", headline):\n",
    "        return False\n",
    "    # C) grammar/spelling check\n",
    "    errs = tool.check(headline)\n",
    "    if len(errs) > 2:\n",
    "        return False\n",
    "    # D) fluency via perplexity\n",
    "    ppl = perplexity(headline)\n",
    "    if ppl > 100:\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b9cc462",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m df = pd.read_csv(csv_path)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 2) apply the quality heuristic\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mis_high_quality\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m].apply(is_high_quality)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 3) report\u001b[39;00m\n\u001b[32m     11\u001b[39m total  = \u001b[38;5;28mlen\u001b[39m(df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/env/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[32m   4918\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4919\u001b[39m         func,\n\u001b[32m   4920\u001b[39m         convert_dtype=convert_dtype,\n\u001b[32m   4921\u001b[39m         by_row=by_row,\n\u001b[32m   4922\u001b[39m         args=args,\n\u001b[32m   4923\u001b[39m         kwargs=kwargs,\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m     ).apply()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/env/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_standard()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/env/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = obj._map_values(\n\u001b[32m   1508\u001b[39m     mapper=curried, na_action=action, convert=\u001b[38;5;28mself\u001b[39m.convert_dtype\n\u001b[32m   1509\u001b[39m )\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/env/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/env/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer(values, mapper, convert=convert)\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mis_high_quality\u001b[39m\u001b[34m(headline)\u001b[39m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# D) fluency via perplexity\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m ppl = perplexity(headline)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ppl > \u001b[32m100\u001b[39m:             \u001b[38;5;66;03m# threshold you can tune\u001b[39;00m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mperplexity\u001b[39m\u001b[34m(sentence)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mperplexity\u001b[39m(sentence):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     inputs = tok_pp(sentence, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     18\u001b[39m         out = model_pp(**inputs, labels=inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:822\u001b[39m, in \u001b[36mBatchEncoding.to\u001b[39m\u001b[34m(self, device, non_blocking)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[32m    818\u001b[39m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[32m    819\u001b[39m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    821\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = {\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m         k: v.to(device=device, non_blocking=non_blocking) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[32m    823\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data.items()\n\u001b[32m    824\u001b[39m     }\n\u001b[32m    825\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    826\u001b[39m     logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. This is not supported.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/env/lib/python3.12/site-packages/torch/cuda/__init__.py:363\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    359\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    360\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    361\u001b[39m     )\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    366\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    367\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) load your full synthetic set\n",
    "csv_path = \"../data/synthetic_data_generate.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 2) apply the quality heuristic\n",
    "df[\"is_high_quality\"] = df[\"input\"].apply(is_high_quality)\n",
    "\n",
    "# 3) report\n",
    "total  = len(df)\n",
    "passed = df[\"is_high_quality\"].sum()\n",
    "print(f\"Total examples:     {total}\")\n",
    "print(f\"High-quality pass:  {passed} ({passed/total:.1%})\")\n",
    "\n",
    "# 4) save only the clean ones\n",
    "df_clean = df[df[\"is_high_quality\"]].drop(columns=\"is_high_quality\")\n",
    "clean_path = \"../data/synthetic_data_generate_clean.csv\"\n",
    "df_clean.to_csv(clean_path, index=False)\n",
    "print(f\"Saved {len(df_clean)} clean examples to {clean_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798ef7f0",
   "metadata": {},
   "source": [
    "### Saving the final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8741a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "df = pd.DataFrame(synthetic_data)\n",
    "csv_path = \"../data/synthetic_data_generate.csv\"\n",
    "\n",
    "# Check if file exists and append if it does\n",
    "if os.path.exists(csv_path):\n",
    "    # Load existing data\n",
    "    existing_df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Combine existing and new data\n",
    "    combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    \n",
    "    # Save the combined data\n",
    "    combined_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Appended {len(df)} new entries to existing data. Total entries: {len(combined_df)}\")\n",
    "else:\n",
    "    # If file doesn't exist yet, just save the new data\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Created new file with {len(df)} entries\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
