{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92fdfcb6",
   "metadata": {},
   "source": [
    "# 1.2 Synthetic Data Generation\n",
    "Main logic for my generation:\n",
    "1. Have a defined sentiment for all of my prompting as a baseline for generating the input and output\n",
    "    - The generation is a diversed mix of zero shot and few shot prompting on the model to strike a balance between the variety of the prompts\n",
    "    - Prompt is requested to generate of around 5-30 words which covers the 25% to 75% percentile to ensure a diverse but not obsecure range of data is generated\n",
    "    - The direct request to generate inputs of a certain sentiment ensures the initial generation to already have a more or less accurate output\n",
    "    - This is done as I realise that the model is rather weak and requires extensive handhelding in generation\n",
    "2. Have a very small sample(of about 30 samples) of real life data inputs from my peers to make the data more diverse again and also provide a sense of human authenticity to it\n",
    "3. Evaluation is done with a heuristic to make sure the data generated is of certain quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e4708",
   "metadata": {},
   "source": [
    "## Main Logic to Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af75d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import re\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4ea0f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0e66eb",
   "metadata": {},
   "source": [
    "### Function to generate user inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6219dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_input(shot, sentiment, type_of_input):\n",
    "    \"\"\"\n",
    "    Generate user input for the model based on the type of input and sentiment.\n",
    "    \n",
    "    Args:\n",
    "        shot (int): either 0 or 1, indicating whether to provide an example or not.\n",
    "        sentiment (str): The sentiment of the headline (neutral, positive, negative).\n",
    "        type_of_input (str): The type of input to generate (e.g., 'headline').\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated input string.\n",
    "    \"\"\"\n",
    "    output = f\"Generate a financial {type_of_input} with {sentiment} sentiment. \"\n",
    "    output += f\"The {type_of_input} should be 5-30 words, factual in tone. \"\n",
    "    \n",
    "    if shot == 0:\n",
    "        output += f\"Example: this is the {type_of_input} you generated\"\n",
    "        return output\n",
    "    \n",
    "    examples = {\n",
    "        \"neutral\": \"Example: A Sexist Joke Cost Ken Fisher $4 Billion in Assets. He Still Runs $121 Billion.\\n\",\n",
    "        \"positive\": \"Example: Western Union will be working with MercadoLibre, the South American eCommerce giant, so digital remittances can be sent in Mexico.\\n\",\n",
    "        \"negative\": \"Example: The app will be delisted from Apple's App Store on Oct. 5.\\n\"\n",
    "    }\n",
    "    \n",
    "    output += examples.get(sentiment, \"\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f88dda",
   "metadata": {},
   "source": [
    "### Function to generate a input for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e2d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_input(prompt):\n",
    "    \"\"\"\n",
    "    Generate synthetic data using the provided prompt with the normal model and tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        prompt (list or str): The input prompt for the model.\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated text from the model.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs, \n",
    "        max_new_tokens=200,\n",
    "        use_cache=True,\n",
    "        temperature=0.9, \n",
    "        min_p=0.1\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c37cb33",
   "metadata": {},
   "source": [
    "### Function to extract out the exact headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f4d81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to clean the generated text\n",
    "def extract_headline(generated_text):\n",
    "    \"\"\"\n",
    "    Extract a clean headline from the generated text.\n",
    "    \n",
    "    Args:\n",
    "        generated_text (str): Raw generated text from the model\n",
    "        \n",
    "    Returns:\n",
    "        str: Clean headline without formatting\n",
    "    \"\"\"\n",
    "    # Remove all EOT tags (in various forms)\n",
    "    generated_text = re.sub(r'<\\|eot(?:_id)?\\|>', '', generated_text)\n",
    "    \n",
    "    # Look for content after assistant tag\n",
    "    if \"<|start_header_id|>assistant<|end_header_id|>\" in generated_text:\n",
    "        content = generated_text.split(\"<|start_header_id|>assistant<|end_header_id|>\")[1].strip()\n",
    "    else:\n",
    "        content = generated_text\n",
    "    \n",
    "    # Find numbered headlines (like \"1. Headline text\")\n",
    "    lines = content.split('\\n')\n",
    "    for line in lines:\n",
    "        # Match lines like \"1. Headline\" or \"1. **Headline**\"\n",
    "        match = re.search(r'\\d+\\.\\s+(?:\\*\\*)?([^*\\n]+)(?:\\*\\*)?', line)\n",
    "        if match:\n",
    "            headline = match.group(1).strip()\n",
    "            # Remove any remaining special tokens\n",
    "            headline = re.sub(r'<\\|[^|]+\\|>', '', headline)\n",
    "            return headline\n",
    "    \n",
    "    # If no numbered headlines found, just return first non-empty line\n",
    "    for line in lines:\n",
    "        if line.strip() and not line.startswith(\"<|\") and not line.startswith(\"Here are\"):\n",
    "            # Remove any remaining special tokens\n",
    "            clean_line = re.sub(r'<\\|[^|]+\\|>', '', line.strip())\n",
    "            return clean_line\n",
    "    \n",
    "    # Clean the content as a last resort\n",
    "    clean_content = re.sub(r'<\\|[^|]+\\|>', '', content.strip())\n",
    "    return clean_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e6526",
   "metadata": {},
   "source": [
    "### Main Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbcc3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating  0\n",
      "U.S. GDP Growth Slows to 2.3% in Q2, Reaching 61.4 Trillion Dollar Mark\n",
      "generating  1\n",
      "US Job Market Grows, Consumer Prices Rise Amidst Slow Economic Growth\n",
      "Done generating lol\n"
     ]
    }
   ],
   "source": [
    "sentiments = [\"neutral\", \"positive\", \"negative\"]\n",
    "type_of_shot = [0, 1]\n",
    "\n",
    "synthetic_data = []\n",
    "n = 0  # Counter for the number of generated headlines\n",
    "\n",
    "for sentiment in sentiments:\n",
    "    for shot in type_of_shot:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a market news generator. Generate ONLY the headline text. Do NOT include any introductions, explanations, or formatting instructions. Do NOT write phrases like 'Here's a headline' or 'Market headline:'. Just output the clean headline text directly.\"},\n",
    "            {\"role\": \"user\", \"content\": generate_user_input(shot, sentiment, \"headline\")},\n",
    "        ]\n",
    "\n",
    "        i = 0\n",
    "        \n",
    "        while i < 79:\n",
    "            if n > 470:\n",
    "                break\n",
    "            print(\"generating \", n)\n",
    "            generated_text = generate_synthetic_input(messages)\n",
    "        \n",
    "            # Extract just the headline\n",
    "            headline = extract_headline(generated_text)\n",
    "            \n",
    "            synthetic_data.append({\n",
    "                    \"input\": headline,\n",
    "                    \"output\": sentiment,\n",
    "                    \"instruction\": \"Base on the sentiment of the headline, classify it as neutral, positive, or negative.\"\n",
    "                })\n",
    "            print(headline)\n",
    "            i += 1\n",
    "            n += 1\n",
    "\n",
    "print(\"Done generating lol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b7457c",
   "metadata": {},
   "source": [
    "## Validation of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87925bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import language_tool_python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 0) select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1) grammar checker\n",
    "tool = language_tool_python.LanguageTool(\"en-US\")\n",
    "\n",
    "# 2) load LM for scoring\n",
    "tok_pp = AutoTokenizer.from_pretrained(\"unsloth/llama-3.2-1b-instruct\")\n",
    "model_pp = AutoModelForCausalLM.from_pretrained(\n",
    "    \"unsloth/llama-3.2-1b-instruct\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ").to(device).eval()\n",
    "\n",
    "def perplexity(sentence):\n",
    "    enc = tok_pp(sentence, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model_pp(**enc, labels=enc[\"input_ids\"])\n",
    "    return torch.exp(out.loss).item()\n",
    "\n",
    "def is_high_quality(headline):\n",
    "    # A) length heuristics\n",
    "    w = headline.split()\n",
    "    if len(w) < 5 or len(w) > 30:\n",
    "        return False\n",
    "    # B) ban URLs or stray tokens\n",
    "    if re.search(r\"http[s]?://|\\<\\|[^\\|]+\\|\\>\", headline):\n",
    "        return False\n",
    "    # C) grammar/spelling check\n",
    "    errs = tool.check(headline)\n",
    "    if len(errs) > 2:\n",
    "        return False\n",
    "    # D) fluency via perplexity\n",
    "    ppl = perplexity(headline)\n",
    "    if ppl > 120:\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9cc462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e92bc652da8e442bb897df51829fa054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "quality:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ row 0 done in 4.40s\n",
      "→ row 1 done in 0.33s\n",
      "→ row 2 done in 0.38s\n",
      "→ row 3 done in 0.43s\n",
      "→ row 4 done in 0.30s\n",
      "→ row 5 done in 0.32s\n",
      "→ row 6 done in 0.26s\n",
      "→ row 7 done in 0.23s\n",
      "→ row 8 done in 0.27s\n",
      "→ row 9 done in 0.19s\n",
      "→ row 10 done in 0.38s\n",
      "→ row 11 done in 0.24s\n",
      "→ row 12 done in 0.47s\n",
      "→ row 13 done in 0.29s\n",
      "→ row 14 done in 0.29s\n",
      "→ row 15 done in 0.20s\n",
      "→ row 16 done in 0.28s\n",
      "→ row 17 done in 0.31s\n",
      "→ row 18 done in 0.32s\n",
      "→ row 19 done in 0.29s\n",
      "→ row 20 done in 0.34s\n",
      "→ row 21 done in 0.33s\n",
      "→ row 22 done in 0.30s\n",
      "→ row 23 done in 0.28s\n",
      "→ row 24 done in 0.27s\n",
      "→ row 25 done in 0.30s\n",
      "→ row 26 done in 0.23s\n",
      "→ row 27 done in 0.35s\n",
      "→ row 28 done in 0.33s\n",
      "→ row 29 done in 0.31s\n",
      "→ row 30 done in 0.28s\n",
      "→ row 31 done in 0.15s\n",
      "→ row 32 done in 0.23s\n",
      "→ row 33 done in 0.37s\n",
      "→ row 34 done in 0.20s\n",
      "→ row 35 done in 0.30s\n",
      "→ row 36 done in 0.27s\n",
      "→ row 37 done in 0.25s\n",
      "→ row 38 done in 0.27s\n",
      "→ row 39 done in 0.24s\n",
      "→ row 40 done in 0.27s\n",
      "→ row 41 done in 0.23s\n",
      "→ row 42 done in 0.20s\n",
      "→ row 43 done in 0.27s\n",
      "→ row 44 done in 0.22s\n",
      "→ row 45 done in 0.31s\n",
      "→ row 46 done in 0.28s\n",
      "→ row 47 done in 0.21s\n",
      "→ row 48 done in 0.25s\n",
      "→ row 49 done in 0.21s\n",
      "→ row 50 done in 0.23s\n",
      "→ row 51 done in 0.21s\n",
      "→ row 52 done in 0.22s\n",
      "→ row 53 done in 0.24s\n",
      "→ row 54 done in 0.19s\n",
      "→ row 55 done in 0.24s\n",
      "→ row 56 done in 0.24s\n",
      "→ row 57 done in 0.26s\n",
      "→ row 58 done in 0.24s\n",
      "→ row 59 done in 0.35s\n",
      "→ row 60 done in 0.18s\n",
      "→ row 61 done in 0.37s\n",
      "→ row 62 done in 0.09s\n",
      "→ row 63 done in 0.56s\n",
      "→ row 64 done in 0.45s\n",
      "→ row 65 done in 0.38s\n",
      "→ row 66 done in 0.39s\n",
      "→ row 67 done in 0.32s\n",
      "→ row 68 done in 0.25s\n",
      "→ row 69 done in 0.24s\n",
      "→ row 70 done in 0.26s\n",
      "→ row 71 done in 0.26s\n",
      "→ row 72 done in 0.33s\n",
      "→ row 73 done in 0.19s\n",
      "→ row 74 done in 0.27s\n",
      "→ row 75 done in 0.41s\n",
      "→ row 76 done in 0.16s\n",
      "→ row 77 done in 0.25s\n",
      "→ row 78 done in 0.21s\n",
      "→ row 79 done in 0.26s\n",
      "→ row 80 done in 0.31s\n",
      "→ row 81 done in 0.24s\n",
      "→ row 82 done in 0.41s\n",
      "→ row 83 done in 0.29s\n",
      "→ row 84 done in 0.31s\n",
      "→ row 85 done in 0.20s\n",
      "→ row 86 done in 0.21s\n",
      "→ row 87 done in 0.22s\n",
      "→ row 88 done in 0.29s\n",
      "→ row 89 done in 0.21s\n",
      "→ row 90 done in 0.28s\n",
      "→ row 91 done in 0.22s\n",
      "→ row 92 done in 0.29s\n",
      "→ row 93 done in 0.31s\n",
      "→ row 94 done in 0.18s\n",
      "→ row 95 done in 0.19s\n",
      "→ row 96 done in 0.43s\n",
      "→ row 97 done in 0.13s\n",
      "→ row 98 done in 0.18s\n",
      "→ row 99 done in 0.50s\n",
      "→ row 100 done in 0.23s\n",
      "→ row 101 done in 0.27s\n",
      "→ row 102 done in 0.25s\n",
      "→ row 103 done in 0.37s\n",
      "→ row 104 done in 0.40s\n",
      "→ row 105 done in 0.23s\n",
      "→ row 106 done in 0.39s\n",
      "→ row 107 done in 0.27s\n",
      "→ row 108 done in 0.25s\n",
      "→ row 109 done in 0.25s\n",
      "→ row 110 done in 0.18s\n",
      "→ row 111 done in 0.37s\n",
      "→ row 112 done in 0.23s\n",
      "→ row 113 done in 0.26s\n",
      "→ row 114 done in 0.22s\n",
      "→ row 115 done in 0.42s\n",
      "→ row 116 done in 0.26s\n",
      "→ row 117 done in 0.30s\n",
      "→ row 118 done in 0.27s\n",
      "→ row 119 done in 0.38s\n",
      "→ row 120 done in 0.35s\n",
      "→ row 121 done in 0.37s\n",
      "→ row 122 done in 0.32s\n",
      "→ row 123 done in 0.27s\n",
      "→ row 124 done in 0.25s\n",
      "→ row 125 done in 0.30s\n",
      "→ row 126 done in 0.19s\n",
      "→ row 127 done in 0.23s\n",
      "→ row 128 done in 0.35s\n",
      "→ row 129 done in 0.21s\n",
      "→ row 130 done in 0.31s\n",
      "→ row 131 done in 0.17s\n",
      "→ row 132 done in 0.23s\n",
      "→ row 133 done in 0.28s\n",
      "→ row 134 done in 0.14s\n",
      "→ row 135 done in 0.26s\n",
      "→ row 136 done in 0.36s\n",
      "→ row 137 done in 0.45s\n",
      "→ row 138 done in 0.51s\n",
      "→ row 139 done in 0.34s\n",
      "→ row 140 done in 0.35s\n",
      "→ row 141 done in 0.20s\n",
      "→ row 142 done in 0.22s\n",
      "→ row 143 done in 0.22s\n",
      "→ row 144 done in 0.24s\n",
      "→ row 145 done in 0.26s\n",
      "→ row 146 done in 0.30s\n",
      "→ row 147 done in 0.25s\n",
      "→ row 148 done in 0.46s\n",
      "→ row 149 done in 0.23s\n",
      "→ row 150 done in 0.25s\n",
      "→ row 151 done in 0.26s\n",
      "→ row 152 done in 0.25s\n",
      "→ row 153 done in 0.20s\n",
      "→ row 154 done in 0.26s\n",
      "→ row 155 done in 0.31s\n",
      "→ row 156 done in 0.24s\n",
      "→ row 157 done in 0.27s\n",
      "→ row 158 done in 0.13s\n",
      "→ row 159 done in 0.27s\n",
      "→ row 160 done in 0.24s\n",
      "→ row 161 done in 0.24s\n",
      "→ row 162 done in 0.28s\n",
      "→ row 163 done in 0.26s\n",
      "→ row 164 done in 0.25s\n",
      "→ row 165 done in 0.20s\n",
      "→ row 166 done in 0.25s\n",
      "→ row 167 done in 0.16s\n",
      "→ row 168 done in 0.30s\n",
      "→ row 169 done in 0.31s\n",
      "→ row 170 done in 0.34s\n",
      "→ row 171 done in 0.23s\n",
      "→ row 172 done in 0.25s\n",
      "→ row 173 done in 0.33s\n",
      "→ row 174 done in 0.28s\n",
      "→ row 175 done in 0.16s\n",
      "→ row 176 done in 0.28s\n",
      "→ row 177 done in 0.34s\n",
      "→ row 178 done in 0.32s\n",
      "→ row 179 done in 0.27s\n",
      "→ row 180 done in 0.32s\n",
      "→ row 181 done in 0.38s\n",
      "→ row 182 done in 0.30s\n",
      "→ row 183 done in 0.27s\n",
      "→ row 184 done in 0.24s\n",
      "→ row 185 done in 0.24s\n",
      "→ row 186 done in 0.36s\n",
      "→ row 187 done in 0.32s\n",
      "→ row 188 done in 0.26s\n",
      "→ row 189 done in 0.21s\n",
      "→ row 190 done in 0.23s\n",
      "→ row 191 done in 0.32s\n",
      "→ row 192 done in 0.21s\n",
      "→ row 193 done in 0.23s\n",
      "→ row 194 done in 0.26s\n",
      "→ row 195 done in 0.24s\n",
      "→ row 196 done in 0.20s\n",
      "→ row 197 done in 0.26s\n",
      "→ row 198 done in 0.34s\n",
      "→ row 199 done in 0.23s\n",
      "→ row 200 done in 0.24s\n",
      "→ row 201 done in 0.31s\n",
      "→ row 202 done in 0.38s\n",
      "→ row 203 done in 0.26s\n",
      "→ row 204 done in 0.21s\n",
      "→ row 205 done in 0.26s\n",
      "→ row 206 done in 0.18s\n",
      "→ row 207 done in 0.15s\n",
      "→ row 208 done in 0.25s\n",
      "→ row 209 done in 0.42s\n",
      "→ row 210 done in 0.38s\n",
      "→ row 211 done in 0.26s\n",
      "→ row 212 done in 0.26s\n",
      "→ row 213 done in 0.32s\n",
      "→ row 214 done in 0.31s\n",
      "→ row 215 done in 0.25s\n",
      "→ row 216 done in 0.27s\n",
      "→ row 217 done in 0.23s\n",
      "→ row 218 done in 0.24s\n",
      "→ row 219 done in 0.23s\n",
      "→ row 220 done in 0.19s\n",
      "→ row 221 done in 0.23s\n",
      "→ row 222 done in 0.31s\n",
      "→ row 223 done in 0.35s\n",
      "→ row 224 done in 0.25s\n",
      "→ row 225 done in 0.23s\n",
      "→ row 226 done in 0.18s\n",
      "→ row 227 done in 0.25s\n",
      "→ row 228 done in 0.21s\n",
      "→ row 229 done in 0.18s\n",
      "→ row 230 done in 0.25s\n",
      "→ row 231 done in 0.19s\n",
      "→ row 232 done in 0.29s\n",
      "→ row 233 done in 0.20s\n",
      "→ row 234 done in 0.25s\n",
      "→ row 235 done in 0.18s\n",
      "→ row 236 done in 0.20s\n",
      "→ row 237 done in 0.22s\n",
      "→ row 238 done in 0.16s\n",
      "→ row 239 done in 0.26s\n",
      "→ row 240 done in 0.25s\n",
      "→ row 241 done in 0.16s\n",
      "→ row 242 done in 0.24s\n",
      "→ row 243 done in 0.37s\n",
      "→ row 244 done in 0.28s\n",
      "→ row 245 done in 0.31s\n",
      "→ row 246 done in 0.31s\n",
      "→ row 247 done in 0.19s\n",
      "→ row 248 done in 0.21s\n",
      "→ row 249 done in 0.19s\n",
      "→ row 250 done in 0.34s\n",
      "→ row 251 done in 0.19s\n",
      "→ row 252 done in 0.39s\n",
      "→ row 253 done in 0.28s\n",
      "→ row 254 done in 0.26s\n",
      "→ row 255 done in 0.22s\n",
      "→ row 256 done in 0.25s\n",
      "→ row 257 done in 0.20s\n",
      "→ row 258 done in 0.22s\n",
      "→ row 259 done in 0.26s\n",
      "→ row 260 done in 0.19s\n",
      "→ row 261 done in 0.21s\n",
      "→ row 262 done in 0.27s\n",
      "→ row 263 done in 0.30s\n",
      "→ row 264 done in 0.24s\n",
      "→ row 265 done in 0.22s\n",
      "→ row 266 done in 0.38s\n",
      "→ row 267 done in 0.22s\n",
      "→ row 268 done in 0.26s\n",
      "→ row 269 done in 0.24s\n",
      "→ row 270 done in 0.37s\n",
      "→ row 271 done in 0.18s\n",
      "→ row 272 done in 0.27s\n",
      "→ row 273 done in 0.25s\n",
      "→ row 274 done in 0.18s\n",
      "→ row 275 done in 0.23s\n",
      "→ row 276 done in 0.20s\n",
      "→ row 277 done in 0.27s\n",
      "→ row 278 done in 0.33s\n",
      "→ row 279 done in 0.37s\n",
      "→ row 280 done in 0.26s\n",
      "→ row 281 done in 0.30s\n",
      "→ row 282 done in 0.43s\n",
      "→ row 283 done in 0.27s\n",
      "→ row 284 done in 0.15s\n",
      "→ row 285 done in 0.36s\n",
      "→ row 286 done in 0.23s\n",
      "→ row 287 done in 0.19s\n",
      "→ row 288 done in 0.26s\n",
      "→ row 289 done in 0.12s\n",
      "→ row 290 done in 0.19s\n",
      "→ row 291 done in 0.21s\n",
      "→ row 292 done in 0.19s\n",
      "→ row 293 done in 0.23s\n",
      "→ row 294 done in 0.27s\n",
      "→ row 295 done in 0.25s\n",
      "→ row 296 done in 0.16s\n",
      "→ row 297 done in 0.25s\n",
      "→ row 298 done in 0.22s\n",
      "→ row 299 done in 0.25s\n",
      "→ row 300 done in 0.24s\n",
      "→ row 301 done in 0.27s\n",
      "→ row 302 done in 0.22s\n",
      "→ row 303 done in 0.27s\n",
      "→ row 304 done in 0.29s\n",
      "→ row 305 done in 0.23s\n",
      "→ row 306 done in 0.13s\n",
      "→ row 307 done in 0.25s\n",
      "→ row 308 done in 0.21s\n",
      "→ row 309 done in 0.30s\n",
      "→ row 310 done in 0.16s\n",
      "→ row 311 done in 0.23s\n",
      "→ row 312 done in 0.28s\n",
      "→ row 313 done in 0.24s\n",
      "→ row 314 done in 0.33s\n",
      "→ row 315 done in 0.24s\n",
      "→ row 316 done in 0.33s\n",
      "→ row 317 done in 0.21s\n",
      "→ row 318 done in 0.20s\n",
      "→ row 319 done in 0.20s\n",
      "→ row 320 done in 0.24s\n",
      "→ row 321 done in 0.23s\n",
      "→ row 322 done in 0.37s\n",
      "→ row 323 done in 0.38s\n",
      "→ row 324 done in 0.29s\n",
      "→ row 325 done in 0.32s\n",
      "→ row 326 done in 0.28s\n",
      "→ row 327 done in 0.22s\n",
      "→ row 328 done in 0.20s\n",
      "→ row 329 done in 0.19s\n",
      "→ row 330 done in 0.29s\n",
      "→ row 331 done in 0.30s\n",
      "→ row 332 done in 0.14s\n",
      "→ row 333 done in 0.30s\n",
      "→ row 334 done in 0.24s\n",
      "→ row 335 done in 0.24s\n",
      "→ row 336 done in 0.40s\n",
      "→ row 337 done in 0.24s\n",
      "→ row 338 done in 0.30s\n",
      "→ row 339 done in 0.24s\n",
      "→ row 340 done in 0.45s\n",
      "→ row 341 done in 0.23s\n",
      "→ row 342 done in 0.36s\n",
      "→ row 343 done in 0.34s\n",
      "→ row 344 done in 0.19s\n",
      "→ row 345 done in 0.21s\n",
      "→ row 346 done in 0.25s\n",
      "→ row 347 done in 0.24s\n",
      "→ row 348 done in 0.28s\n",
      "→ row 349 done in 0.31s\n",
      "→ row 350 done in 0.21s\n",
      "→ row 351 done in 0.26s\n",
      "→ row 352 done in 0.31s\n",
      "→ row 353 done in 0.23s\n",
      "→ row 354 done in 0.38s\n",
      "→ row 355 done in 0.22s\n",
      "→ row 356 done in 0.25s\n",
      "→ row 357 done in 0.25s\n",
      "→ row 358 done in 0.31s\n",
      "→ row 359 done in 0.23s\n",
      "→ row 360 done in 0.30s\n",
      "→ row 361 done in 0.35s\n",
      "→ row 362 done in 0.28s\n",
      "→ row 363 done in 0.23s\n",
      "→ row 364 done in 0.23s\n",
      "→ row 365 done in 0.29s\n",
      "→ row 366 done in 0.38s\n",
      "→ row 367 done in 0.29s\n",
      "→ row 368 done in 0.31s\n",
      "→ row 369 done in 0.19s\n",
      "→ row 370 done in 0.25s\n",
      "→ row 371 done in 0.33s\n",
      "→ row 372 done in 0.26s\n",
      "→ row 373 done in 0.32s\n",
      "→ row 374 done in 0.16s\n",
      "→ row 375 done in 0.33s\n",
      "→ row 376 done in 0.24s\n",
      "→ row 377 done in 0.31s\n",
      "→ row 378 done in 0.27s\n",
      "→ row 379 done in 0.18s\n",
      "→ row 380 done in 0.37s\n",
      "→ row 381 done in 0.34s\n",
      "→ row 382 done in 0.23s\n",
      "→ row 383 done in 0.28s\n",
      "→ row 384 done in 0.24s\n",
      "→ row 385 done in 0.24s\n",
      "→ row 386 done in 0.25s\n",
      "→ row 387 done in 0.26s\n",
      "→ row 388 done in 0.30s\n",
      "→ row 389 done in 0.28s\n",
      "→ row 390 done in 0.23s\n",
      "→ row 391 done in 0.26s\n",
      "→ row 392 done in 0.25s\n",
      "→ row 393 done in 0.27s\n",
      "→ row 394 done in 0.24s\n",
      "→ row 395 done in 0.26s\n",
      "→ row 396 done in 0.27s\n",
      "→ row 397 done in 0.32s\n",
      "→ row 398 done in 0.26s\n",
      "→ row 399 done in 0.26s\n",
      "→ row 400 done in 0.36s\n",
      "→ row 401 done in 0.30s\n",
      "→ row 402 done in 0.26s\n",
      "→ row 403 done in 0.24s\n",
      "→ row 404 done in 0.24s\n",
      "→ row 405 done in 0.22s\n",
      "→ row 406 done in 0.26s\n",
      "→ row 407 done in 0.27s\n",
      "→ row 408 done in 0.30s\n",
      "→ row 409 done in 0.16s\n",
      "→ row 410 done in 0.29s\n",
      "→ row 411 done in 0.26s\n",
      "→ row 412 done in 0.28s\n",
      "→ row 413 done in 0.37s\n",
      "→ row 414 done in 0.26s\n",
      "→ row 415 done in 0.25s\n",
      "→ row 416 done in 0.28s\n",
      "→ row 417 done in 0.24s\n",
      "→ row 418 done in 0.23s\n",
      "→ row 419 done in 0.21s\n",
      "→ row 420 done in 0.36s\n",
      "→ row 421 done in 0.35s\n",
      "→ row 422 done in 0.17s\n",
      "→ row 423 done in 0.30s\n",
      "→ row 424 done in 0.22s\n",
      "→ row 425 done in 0.28s\n",
      "→ row 426 done in 0.26s\n",
      "→ row 427 done in 0.20s\n",
      "→ row 428 done in 0.24s\n",
      "→ row 429 done in 0.23s\n",
      "→ row 430 done in 0.25s\n",
      "→ row 431 done in 0.37s\n",
      "→ row 432 done in 0.24s\n",
      "→ row 433 done in 0.26s\n",
      "→ row 434 done in 0.19s\n",
      "→ row 435 done in 0.23s\n",
      "→ row 436 done in 0.32s\n",
      "→ row 437 done in 0.22s\n",
      "→ row 438 done in 0.19s\n",
      "→ row 439 done in 0.24s\n",
      "→ row 440 done in 0.25s\n",
      "→ row 441 done in 0.26s\n",
      "→ row 442 done in 0.27s\n",
      "→ row 443 done in 0.27s\n",
      "→ row 444 done in 0.30s\n",
      "→ row 445 done in 0.22s\n",
      "→ row 446 done in 0.28s\n",
      "→ row 447 done in 0.22s\n",
      "→ row 448 done in 0.25s\n",
      "→ row 449 done in 0.23s\n",
      "→ row 450 done in 0.19s\n",
      "→ row 451 done in 0.24s\n",
      "→ row 452 done in 0.25s\n",
      "→ row 453 done in 0.25s\n",
      "→ row 454 done in 0.30s\n",
      "→ row 455 done in 0.21s\n",
      "→ row 456 done in 0.24s\n",
      "→ row 457 done in 0.22s\n",
      "→ row 458 done in 0.27s\n",
      "→ row 459 done in 0.26s\n",
      "→ row 460 done in 0.32s\n",
      "→ row 461 done in 0.23s\n",
      "→ row 462 done in 0.22s\n",
      "→ row 463 done in 0.29s\n",
      "→ row 464 done in 0.18s\n",
      "→ row 465 done in 0.29s\n",
      "→ row 466 done in 0.27s\n",
      "→ row 467 done in 0.26s\n",
      "→ row 468 done in 0.28s\n",
      "→ row 469 done in 0.13s\n",
      "→ row 470 done in 0.14s\n",
      "→ row 471 done in 0.19s\n",
      "→ row 472 done in 0.00s\n",
      "→ row 473 done in 0.17s\n",
      "→ row 474 done in 0.09s\n",
      "→ row 475 done in 0.15s\n",
      "→ row 476 done in 0.19s\n",
      "→ row 477 done in 0.10s\n",
      "→ row 478 done in 0.11s\n",
      "→ row 479 done in 0.16s\n",
      "→ row 480 done in 0.10s\n",
      "→ row 481 done in 0.16s\n",
      "→ row 482 done in 0.10s\n",
      "→ row 483 done in 0.10s\n",
      "→ row 484 done in 0.11s\n",
      "→ row 485 done in 0.11s\n",
      "→ row 486 done in 0.10s\n",
      "→ row 487 done in 0.09s\n",
      "→ row 488 done in 0.10s\n",
      "→ row 489 done in 0.14s\n",
      "→ row 490 done in 0.10s\n",
      "→ row 491 done in 0.10s\n",
      "→ row 492 done in 0.10s\n",
      "→ row 493 done in 0.10s\n",
      "→ row 494 done in 0.16s\n",
      "→ row 495 done in 0.19s\n",
      "→ row 496 done in 0.19s\n",
      "→ row 497 done in 0.09s\n",
      "→ row 498 done in 0.12s\n",
      "→ row 499 done in 0.09s\n",
      "Total examples:     500\n",
      "High-quality pass:  479 (95.8%)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "tqdm.pandas()\n",
    "\n",
    "csv_path = \"../data/synthetic_data_generate.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "results = []\n",
    "for idx, text in tqdm(df[\"input\"].items(), total=len(df), desc=\"quality\"):\n",
    "    t0 = time.time()\n",
    "    ok = is_high_quality(text)\n",
    "    results.append(ok)\n",
    "\n",
    "df[\"is_high_quality\"] = results\n",
    "\n",
    "total  = len(df)\n",
    "passed = df[\"is_high_quality\"].sum()\n",
    "print(f\"Total examples:     {total}\")\n",
    "print(f\"High-quality pass:  {passed} ({passed/total:.1%})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798ef7f0",
   "metadata": {},
   "source": [
    "### Saving the final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8741a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "df = pd.DataFrame(synthetic_data)\n",
    "csv_path = \"../data/synthetic_data_generate.csv\"\n",
    "\n",
    "# Check if file exists and append if it does\n",
    "if os.path.exists(csv_path):\n",
    "    # Load existing data\n",
    "    existing_df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Combine existing and new data\n",
    "    combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    \n",
    "    # Save the combined data\n",
    "    combined_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Appended {len(df)} new entries to existing data. Total entries: {len(combined_df)}\")\n",
    "else:\n",
    "    # If file doesn't exist yet, just save the new data\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Created new file with {len(df)} entries\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
